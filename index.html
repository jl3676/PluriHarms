<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="PLURIHARMS: Benchmarking the Full Spectrum of Human Judgments on AI Harm">
  <meta name="description" content="A comprehensive benchmark for evaluating the full spectrum of human judgments on AI-generated harm across diverse perspectives and contexts.">
  <meta name="keywords" content="AI safety, harm evaluation, human judgments, AI ethics, benchmark, machine learning, NeurIPS, AI alignment">
  <meta name="author" content="Jing-Jing Li, Joel Mire, Eve Fleisig, Valentina Pyatkin, Maarten Sap, Sydney Levine">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="PluriHarms">
  <meta property="og:title" content="PLURIHARMS: Benchmarking the Full Spectrum of Human Judgments on AI Harm">
  <meta property="og:description" content="A comprehensive benchmark for evaluating the full spectrum of human judgments on AI-generated harm across diverse perspectives and contexts.">
  <meta property="og:url" content="https://github.com/JingjingLi/PluriHarms">
  <meta property="og:image" content="https://github.com/JingjingLi/PluriHarms/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PLURIHARMS - Research Preview">
  <meta property="article:published_time" content="2025-01-01T00:00:00.000Z">
  <meta property="article:author" content="Jing-Jing Li">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="AI safety">
  <meta property="article:tag" content="AI ethics">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@allenai">
  <meta name="twitter:creator" content="@JingjingLi">
  <meta name="twitter:title" content="PLURIHARMS: Benchmarking the Full Spectrum of Human Judgments on AI Harm">
  <meta name="twitter:description" content="A comprehensive benchmark for evaluating the full spectrum of human judgments on AI-generated harm across diverse perspectives and contexts.">
  <meta name="twitter:image" content="https://github.com/JingjingLi/PluriHarms/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PLURIHARMS - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PLURIHARMS: Benchmarking the Full Spectrum of Human Judgments on AI Harm">
  <meta name="citation_author" content="Li, Jing-Jing">
  <meta name="citation_author" content="Mire, Joel">
  <meta name="citation_author" content="Fleisig, Eve">
  <meta name="citation_author" content="Pyatkin, Valentina">
  <meta name="citation_author" content="Sap, Maarten">
  <meta name="citation_author" content="Levine, Sydney">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="NeurIPS CogInterp Workshop">
  <meta name="citation_pdf_url" content="https://github.com/JingjingLi/PluriHarms/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>PLURIHARMS: Benchmarking the Full Spectrum of Human Judgments on AI Harm</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/svg+xml" href="static/images/favicon.svg">
  <link rel="apple-touch-icon" href="static/images/favicon.svg">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PLURIHARMS: Benchmarking the Full Spectrum of Human Judgments on AI Harm",
    "description": "A comprehensive benchmark for evaluating the full spectrum of human judgments on AI-generated harm across diverse perspectives and contexts.",
    "author": [
      {
        "@type": "Person",
        "name": "Jing-Jing Li",
        "affiliation": {
          "@type": "Organization",
          "name": "UC Berkeley"
        }
      },
      {
        "@type": "Person",
        "name": "Joel Mire",
        "affiliation": {
          "@type": "Organization",
          "name": "Carnegie Mellon University"
        }
      },
      {
        "@type": "Person",
        "name": "Eve Fleisig",
        "affiliation": {
          "@type": "Organization",
          "name": "UC Berkeley"
        }
      },
      {
        "@type": "Person",
        "name": "Valentina Pyatkin",
        "affiliation": {
          "@type": "Organization",
          "name": "Allen Institute for AI"
        }
      },
      {
        "@type": "Person",
        "name": "Maarten Sap",
        "affiliation": [
          {
            "@type": "Organization",
            "name": "Carnegie Mellon University"
          },
          {
            "@type": "Organization",
            "name": "Allen Institute for AI"
          }
        ]
      },
      {
        "@type": "Person",
        "name": "Sydney Levine",
        "affiliation": {
          "@type": "Organization",
          "name": "New York University"
        }
      }
    ],
    "datePublished": "2025-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "NeurIPS CogInterp Workshop"
    },
    "url": "https://github.com/JingjingLi/PluriHarms",
    "image": "https://github.com/JingjingLi/PluriHarms/static/images/social_preview.png",
    "keywords": ["AI safety", "harm evaluation", "human judgments", "AI ethics", "benchmark", "machine learning"],
    "abstract": "PLURIHARMS is a comprehensive benchmark for evaluating the full spectrum of human judgments on AI-generated harm.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://github.com/JingjingLi/PluriHarms"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "AI Safety"
      },
      {
        "@type": "Thing", 
        "name": "AI Ethics"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Allen Institute for AI",
    "url": "https://allenai.org",
    "logo": "https://github.com/JingjingLi/PluriHarms/static/images/favicon.svg",
    "sameAs": [
      "https://twitter.com/allenai",
      "https://github.com/allenai"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>


  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <span class="pluriharms-title">
                <span style="color: #4D91A6;">P</span><span style="color: #5F6FB4;">l</span><span style="color: #7D5AA6;">u</span><span style="color: #A15993;">r</span><span style="color: #C76888;">i</span><span style="color: #D95D5D;">H</span><span style="color: #E17B4C;">a</span><span style="color: #E6A84B;">r</span><span style="color: #B7B958;">m</span><span style="color: #64A36F;">s</span>
              </span>: Benchmarking the Full Spectrum of<br>Human Judgments on AI Harm
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://jl3676.github.io/" target="_blank">Jing-Jing Li</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://joel-mire.github.io/" target="_blank">Joel Mire</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.efleisig.com/" target="_blank">Eve Fleisig</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://valentinapy.github.io/" target="_blank">Valentina Pyatkin</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://maartensap.com/" target="_blank">Maarten Sap</a><sup>2,3</sup>,</span>
              <span class="author-block">
                <a href="https://sites.google.com/site/sydneymlevine/" target="_blank">Sydney Levine</a><sup>4</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>UC Berkeley, <sup>2</sup>Carnegie Mellon University, <sup>3</sup>Allen Institute for AI, <sup>4</sup>Google DeepMind</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark" style="pointer-events: none; opacity: 0.6;">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper (Coming Soon)</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark" style="pointer-events: none; opacity: 0.6;">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark" style="pointer-events: none; opacity: 0.6;">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Data (Coming Soon)</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Current AI safety frameworks, which often treat harmfulness as binary, lack the flexibility to handle borderline cases where humans meaningfully disagree. To build more pluralistic systems, it is essential to move beyond consensus and instead understand where and why disagreements arise. We introduce <span class="pluriharms-title"><span style="color: #4D91A6;">P</span><span style="color: #5F6FB4;">l</span><span style="color: #7D5AA6;">u</span><span style="color: #A15993;">r</span><span style="color: #C76888;">i</span><span style="color: #D95D5D;">H</span><span style="color: #E17B4C;">a</span><span style="color: #E6A84B;">r</span><span style="color: #B7B958;">m</span><span style="color: #64A36F;">s</span></span>, a benchmark designed to systematically study human harm judgments across two key dimensions—the harm axis (benign to harmful) and the agreement axis (agreement to disagreement). Our scalable framework generates prompts that capture diverse AI harms and human values while targeting cases with high disagreement rates, validated by human data. The benchmark includes 150 prompts with 15,000 ratings from 100 human annotators, enriched with demographic and psychological traits and prompt-level features of harmful actions, effects, and values. Our analyses show that prompts that relate to imminent risks and tangible harms amplify perceived harmfulness, while annotator traits (e.g., toxicity experience, education) and their interactions with prompt content explain systematic disagreement. We benchmark AI safety models and alignment methods on <span class="pluriharms-title"><span style="color: #4D91A6;">P</span><span style="color: #5F6FB4;">l</span><span style="color: #7D5AA6;">u</span><span style="color: #A15993;">r</span><span style="color: #C76888;">i</span><span style="color: #D95D5D;">H</span><span style="color: #E17B4C;">a</span><span style="color: #E6A84B;">r</span><span style="color: #B7B958;">m</span><span style="color: #64A36F;">s</span></span>, finding that while personalization significantly improves prediction of human harm judgments, considerable room remains for future progress. By explicitly targeting value diversity and disagreement, our work provides a principled benchmark for moving beyond "one-size-fits-all" safety toward pluralistically safe AI.
          </p>
        </div>
        <div class="notification is-info is-light" style="margin-top: 2rem;">
          <p class="has-text-centered">
            <strong>📢 Paper accepted to NeurIPS 2025 CogInterp Workshop</strong><br>
            <span style="font-size: 0.95em;">Code, data, and full paper will be released upon publication (expected in December, 2026)</span>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@inproceedings{li2025pluriharms,
  title={PLURIHARMS: Benchmarking the Full Spectrum of Human Judgments on AI Harm},
  author={Li, Jing-Jing and Mire, Joel and Fleisig, Eve and Pyatkin, Valentina and Sap, Maarten and Levine, Sydney},
  booktitle={NeurIPS CogInterp Workshop},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
